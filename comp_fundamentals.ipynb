{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc05eb2",
   "metadata": {},
   "source": [
    "# 1. Computation fundamentals\n",
    "\n",
    "There are several sources of errors in computation: \n",
    "\n",
    "- **roundoff**, a consequence of our digital representation of numbers,\n",
    "- **conditioning**, a property of the problem, and\n",
    "- **stability**, a property of the algorithm we use.\n",
    "\n",
    "```{admonition} Additional resource\n",
    ":class: warning\n",
    "Trefethen and Bau's **Numerical Linear Algebra**, Chapters 12-14.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9e0f9",
   "metadata": {},
   "source": [
    "## 1.1 Floating-point arithmetic\n",
    "\n",
    "Computers work in terms of floating-point numbers instead of reals $\\mathbb{R}$. \n",
    "\n",
    "Let's say $\\circ$ is an operator that rounds the real number $x \\in \\mathbb{R}$ to the nearest floating-point number $\\tilde{x}$:\n",
    "\n",
    "$$ \\tilde{x} := \\circ x. $$\n",
    "\n",
    "The _absolute_ error induced by this representation is\n",
    "\n",
    "$$ \\Delta x = \\circ x - x = \\tilde{x} - x, $$\n",
    "and the _relative_ error is\n",
    "\n",
    "$$ \\delta x = \\frac{\\Delta x}{x} = \\frac{\\tilde{x} - x}{x}. $$\n",
    "We can rearrange this as\n",
    "\n",
    "$$ \\tilde{x} = (1 + \\delta x)x. $$\n",
    "\n",
    "The IEEE (Institute of Electrical and Electronics Engineers) standard _guarantees_ that \n",
    "\n",
    "$$ |\\delta x| < \\mu_M = \\tfrac{1}{2}\\varepsilon_M, $$\n",
    "where $\\varepsilon_M$ is **machine precision**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f84bf",
   "metadata": {},
   "source": [
    "```{admonition} Question\n",
    "Where does $\\varepsilon_M$ come from, and what is it in single and double precision arithmetic?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c4eea",
   "metadata": {},
   "source": [
    "Numbers in floating-point arithmetic are represented as\n",
    "\n",
    "$$ \\pm (1 + f) \\cdot 2^n, $$ \n",
    "where $n$ is the **exponent** and $(1+f)$ is the **mantissa**. \n",
    "\n",
    "A fixed number of bits called the binary **precision**, denoted $d$, is then used to store $f$ in base 2:\n",
    "\n",
    "$$ f = \\sum_{i = 1}^d b_i 2^{-i}, \\quad b_i \\in \\{0, 1\\}. $$\n",
    "It's useful to pull out a constant $2^{-d}$ to get\n",
    "\n",
    "$$ f = 2^{-d}\\sum_{i = 1}^{d} b_i 2^{d - i} = 2^{-d}z, $$\n",
    "where $z$ is now an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676dafa",
   "metadata": {},
   "source": [
    "```{admonition} Question\n",
    "What values can $z$ take? How many numbers are there between $2^n$ and $2^{n+1}$? What does that say about the relationship between $\\varepsilon_M$ and $d$?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0d4ba",
   "metadata": {},
   "source": [
    "$z$ takes values in $ z \\in \\{0, 1, 2, \\ldots, 2^{d} - 1\\}$, therefore there are $2^d$ integers between two adjacent powers of $2$ in floating-point arithmetic. From this we can read off that\n",
    "\n",
    "$$ \\varepsilon_M = 2^{-d}. $$\n",
    "In single precision, $d = 23$ bits are used to represent the mantissa, therefore $\\varepsilon_M \\approx 2\\cdot 10^{-7}$ and from $\\mu_M$ we expect each number to be accurate to around 7 digits. For double precision, $d = 52$ and numbers can be trusted to 16 digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0899597",
   "metadata": {},
   "source": [
    "## 1.2 Conditioning of a problem; condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f217e",
   "metadata": {},
   "source": [
    "Conditioning is entirely the property of a problem; it tells you how accurate an anwer you can _expect_ from \"ideal\" (i.e. backwards-stable) algorithms solving it.\n",
    "\n",
    "\n",
    "\"Problem\" in this context is a map:\n",
    "\n",
    "$$ f: X \\to Y, $$\n",
    "- $X$ is the set of input parameters/space _defining_ the problem (all the information you need to solve it),\n",
    "- $Y$ is the space of solutions.\n",
    "\n",
    "$f$ could for example be \n",
    "- a function that doubles its input, $f(x) = 2x$, or\n",
    "- a function that returns the roots of a polynomial defined via a vector $\\mathbb{x}$ of its coefficients, or\n",
    "- a mapping between an initial value problem (say, an ODE and its initial conditions) and its set of solutions.\n",
    "\n",
    "The conditioning of $f$ is the _sensitivity_ of its output $f(x)$ to perturbations in its input $x$. If the perturbation in the input $x$ is $\\delta x$, then the change in the output is $\\delta f := f(x + \\delta x) - f(x)$.\n",
    "\n",
    "The **absolute condition number** is defined as\n",
    "$$ \\tilde{\\kappa} = \\tilde{\\kappa}(x) := \\lim_{\\delta \\to 0} \\sup_{|\\delta x| \\leq \\delta} \\frac{|| \\delta f|| }{|| \\delta x||} := \\sup_{\\delta x} \\frac{|| \\delta f|| }{|| \\delta x||}, $$\n",
    "where the $|| \\cdot ||$ denotes the 2-norm.\n",
    "\n",
    "If $x \\in \\mathbb{C}^n$, $f \\in \\mathbb{C}^m$ are $n$- and $m$-dimensional vectors, respectively (meaning: there are multiple inputs and/or outputs), then we can measure how a perturbation in any of the inputs affects any of the outputs via the **Jacobian** matrix $J_{ij} \\in \\mathbb{C}^{m \\times n}$,\n",
    "$$ \\frac{\\partial f_i}{\\partial x_j} = J_{ij}(x). $$\n",
    "In this case, as $\\tilde{\\kappa}(x) = ||J(x)||$.\n",
    "\n",
    "A more practical quantity is the **relative condition number**, defined as\n",
    "$$ \\kappa(x) := \\sup_{\\delta x} \\frac{||\\delta f||/||f||}{||\\delta x||/||x||} = \\frac{||J(x)||}{||f||/||x||} $$\n",
    "i.e. as the _relative_ change in output for a small _relative_ change in input.\n",
    "\n",
    "For scalar functions $f$, this simplifies to $\\kappa(x) = \\left|\\frac{xf'(x)}{f(x)}\\right|.$\n",
    "\n",
    "```{admonition} Question\n",
    "Why do we care about perturbations in the input? Aren't problems (e.g. functions) mostly defined exactly?\n",
    "```\n",
    "\n",
    "Due to floating-point representation (roundoff) error, there is always at least $\\mu_M$ relative uncertainty/error in the input. \n",
    "\n",
    "```{admonition} Question\n",
    "What does the condition number depend on? Does it depend on the precision (single, double) we use?\n",
    "```\n",
    "\n",
    "The condition number tells you how much this error is amplified by the problem. It is only a function of the problem and the data, and not the algorithm/computer/precision. If the perturbation $\\delta x \\ll 1$, then \n",
    "\n",
    "$$ \\frac{||\\delta f||}{||f||} \\approx \\kappa(x) ||\\delta x||, $$\n",
    "so if $\\kappa = 10^d$, we expect to lose $d$ digits of accuracy in computing $f$ from $x$.\n",
    "\n",
    "We say that a problem is\n",
    "$$\n",
    "\\kappa \\begin{cases} &< 10^3 \\quad \\text{well-conditioned,} \\\\\n",
    "&\\gg 10^3 \\quad \\text{ill-conditioned}. \\end{cases}\n",
    "$$\n",
    "\n",
    "```{admonition} Question\n",
    "What happens if $\\kappa \\approx 1/\\mu_M$?\n",
    "```\n",
    "\n",
    "If $\\kappa \\approx 1/\\mu_M$, we do not expect any useful digits in the answer, and the function $f$ is essentially uncomputable...\n",
    "\n",
    "```{admonition} Question\n",
    "What to do if this happens but we still need to compute $f$ to some useful digits?\n",
    "```\n",
    "... unless we raise the precision and hence lower $\\varepsilon_M$ (e.g. single to double, double to quad).\n",
    "\n",
    "Let's have a look at the conditioning of some common operations.\n",
    "\n",
    "```{admonition} Question\n",
    "What's the condition number of \n",
    "- $f(x) = 2x$,\n",
    "- $f(x) = x^{\\alpha}$,\n",
    "- $f(x_1, x_2) = x_1 - x_2$,\n",
    "- $f(x) = \\sin(x)$ for, say, $x = 10^{100}$,\n",
    "- $f(x) = Ax$ (where $A$ is an $m \\times m$ matrix),\n",
    "- $f$, if $f(x)$ returns the solution of $Ax = b$ given a matrix $A$ and a vector $b$?\n",
    "```\n",
    "1. For $f(x) = 2x$, $J = f' = 2$, so $\\kappa = \\left|\\frac{2x}{2x}\\right| = 1$, always well-conditioned.\n",
    "2. For $f(x) = x^{\\alpha}$, $J = f' \\propto \\alpha$, $\\kappa = |\\alpha|$ is well-conditioned for reasonable powers $\\alpha$,\n",
    "3. Now there are multiple inputs ($n = 2$) but a single output ($m = 1$), so we have $J = [ 1 -1]$, $||J|| = \\sqrt{2},$ and $\\kappa = \\frac{\\sqrt{2}\\sqrt{x_1^2 + x_2^2}}{|x_1 - x_2|}$, which $\\to \\infty$ as $x_1 \\to x_2 \\neq 0$! So this problem becomes ill-conditioned for close-by inputs. This is a well-known phenomenon called **subtractive** or **catastrophic cancellation**.\n",
    "4. $f(x) = \\sin(x)$, $||J|| \\leq 1$, but $\\kappa = \\frac{||J|||x|}{|\\sin(x)|} \\geq |x|$ gets huge with increasing $x$. Note, however, that the absolute convergence number is just $\\leq 1$.\n",
    "5. $f(x) = Ax$, so $J = A$. Then $\\kappa = ||A|| \\frac{||x||}{||Ax||}$. If $A$ is non-singular, $\\kappa \\leq ||A||||A^{-1}||$, since $||x|| = ||A^{-1} Ax|| \\leq ||A^{-1}||||Ax||$. \n",
    "```{admonition} Question\n",
    "When is there an exact equality?\n",
    "```\n",
    "6. Solving a linear system. $Ax = b$, so $f(b) = A^{-1}b$. We can use the previous result (replace $A$ with $A^{-1}$) to get $\\kappa \\leq ||A^{-1}||||A||.$\n",
    "```{admonition} Question\n",
    "What was perturbed in the above calculation of $\\kappa$? Is there another quantity we can vary and define a cond. no. in terms of?\n",
    "```\n",
    "We may also perturb $A$ instead of $b$. If the input is $A$, output is $x$ ($b$ const.), then consider an infinitesimal change in the input, $A + \\delta A$ causing an infinitesimal change in output, $x + \\delta x$:\n",
    "$$ (A + \\delta A)(x + \\delta x) = b, $$\n",
    "$$ Ax + \\delta A x + A \\delta x + \\delta A \\delta x = b. $$\n",
    "Ignore the last term on the LHS (correct to first order) and cancel the $Ax = b$ to get\n",
    "$$ \\delta x = -A^{-1}\\delta A x, $$\n",
    "therefore $\\frac{||\\delta x||}{||x||} \\leq ||A^{-1}||||\\delta A||,$ and $\\kappa \\leq ||A^{-1}||||A||$ again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47520553",
   "metadata": {},
   "source": [
    "## 1.3 Stability\n",
    "\n",
    "Stability is entirely the property of the algorithm used to solve a problem. If an algorithm is stable, this loosely means that it gets the right answer, even if it is not exact.\n",
    "\n",
    "Let's say we have a fixed\n",
    "- problem, $f: X \\to Y$, e.g. $y = \\sin x$ or $y$ solves $Ay = b$,\n",
    "- floating-point repserentation (with a given precision),\n",
    "- algorithm for $f$, a map $\\tilde{f}: X \\to Y$, with steps\n",
    "$$ x \\to \\circ x \\to \\tilde{f} (x). $$\n",
    "\n",
    "Then the relative error of the computation is \n",
    "$$ \\frac{|| \\tilde{f}(x) - f(x)||}{||f(x)||}. $$\n",
    "The algorithm is definitely good if this answer is $\\mathcal{O}(\\mu_M)$ (read: \"of the order of\").\n",
    "```{admonition} Aside\n",
    "Formally, the \"big O\" notation means that there exists a constant $c$ such that answer is $\\leq c\\mu_M$ as $\\mu_M \\to 0$, uniformly over all input data $x \\in X$.\n",
    "```\n",
    "```{admonition} Question\n",
    "Is it reasonable to demand this?\n",
    "```\n",
    "If the problem, however, is ill-conditioned, this is not a reasonable ask. Even if the algorithm were to give an exact answer, the answer will not have a relative error of $\\mathcal{O}(\\mu_M)$.\n",
    "\n",
    "So instead we define and algorithm as **stable** if $\\forall x \\in X$, \n",
    "$$ \\frac{||\\tilde{f}(x) - f(\\tilde{x})||}{||f(\\tilde{x})||} = \\mathcal{O}(\\mu_M) \\quad \\text{for some }\\tilde{x}\\text{ such that}\\quad \\frac{||\\tilde{x} - x||}{||x||} = \\mathcal{O}(\\mu_M).  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040146fb",
   "metadata": {},
   "source": [
    "Meaning that the algorithm gives \"nearly the right answer to nearly the right question\".\n",
    "\n",
    "A stronger requirement for an algorithm is to be **backwards stable**, i.e. $\\forall x \\in X$\n",
    "$$ \\tilde{f}(x) = f(\\tilde{x}) \\quad \\text{for some }\\tilde{x}\\text { such that}\\quad \\frac{||\\tilde{x} - x||}{||x||} = \\mathcal{O}(\\mu_M), $$\n",
    "in words, it is a requirement for the algorithm to give \"exactly the right answer to the nearly right question\".\n",
    "\n",
    "The expression \"backwards stable\" comes from the related concept of **backward error**. Using the above notation, if there is a value $\\tilde{x}$ such that $\\tilde{f}(x) = f(\\tilde{x})$, then the relative backward error is\n",
    "$$\\frac{||\\tilde{x} - x||}{||x||}, $$\n",
    "whereas the absolute backward error is $||\\tilde{x} - x||$. \n",
    "But why is it called \"backward\"? The name comes from the fact that this error measures the deviation from the original input/data $x$ that reproduces the result that was found by the algorithm. [Fig. 1.4.1.](https://tobydriscoll.net/fnc-julia/intro/stability.html#fig-backwarderror) from Driscoll and Braun's Fundamentals of Numerical Computation illustrates the relationship between the absolute error $||\\tilde{f}(x) - f(x)||$ and the absolute backward error $||\\tilde{x} - x||$:\n",
    "```{image} img/backwarderror.svg\n",
    ":name: backwarderror-fig\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "Is the floating-point subtraction, $\\ominus$ backwards stable?\n",
    "```\n",
    "- The exact problem is $f(x_1, x_2) = x_1 - x_2$.\n",
    "- The algorithm is $\\tilde{f}(x_1, x_2) = \\circ{x_1} \\ominus \\circ{x_2} = \\left[x_1(1+\\varepsilon_1) - x_2(1 + \\varepsilon_2)\\right](1 + \\varepsilon_3)$, where $|\\varepsilon_i| \\leq \\mu_M$ for each $i = 1, 2, 3.$\n",
    "- By renaming, write as $x_1(1 + \\varepsilon_4) - x_2(1 + \\varepsilon_5)$, where $|\\varepsilon_j| \\leq 2\\mu_M$ for $j = 4, 5$, which is $f(\\tilde{x}_1, \\tilde{x}_2)$, i.e. exact for some data close to $x_1$, $x_2$. \n",
    "The subtraction algorithm is therefore backwards stable.\n",
    "\n",
    "```{admonition} Question\n",
    "Can backwards stable algorithms produce large relative errors $\\frac{||\\tilde{f}(x) - f(\\tilde{x})||}{||f(\\tilde{x})||}$? If so, how?\n",
    "```\n",
    "Absolutely! The only way this can happen though, is if $\\kappa$ is very large. An important result is that \n",
    "\n",
    "```{admonition} Theorem\n",
    ":class: hint\n",
    "If a backwards stable algorithm using floating-point representation is used to solve problem $f(x)$ with relative condition number $\\kappa(x)$, then the resulting relative error satisfies\n",
    "$$ \\delta := \\frac{||\\tilde{f}(x) - f(x)||}{||f(x)||} = \\mathcal{O}(\\kappa(x)\\mu_M).$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b436e8",
   "metadata": {},
   "source": [
    "**Proof:** Backwards stability means that \n",
    "$$ \\tilde{f}(x) = f(\\tilde{x}) $$\n",
    "for some $\\frac{||x - \\tilde{x}||}{||x||} = \\mathcal{O}(\\mu_M)$.\n",
    "Using this to replace the $\\tilde{f}(x)$ with $f(\\tilde{x})$ in the relative error and using the definition of $\\kappa$ we get  \n",
    "$$ \\delta = \\frac{||f(\\tilde{x}) - f(x)||}{||f(x)||} \\leq \\kappa(x)\\frac{||x - \\tilde{x}||}{||x||} = \\kappa(x)\\mathcal{O}(\\mu_M). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033da29d",
   "metadata": {},
   "source": [
    "Let's revisit the subtraction example to see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d660a87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.992007221626409e-14"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = 3.0000000000006\n",
    "x2 = 3.0000000000005\n",
    " \n",
    "x1 - x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb4623",
   "metadata": {},
   "source": [
    "We see a large relative error (we know the correct answer is $10^{-13}$) in the computed vs. the exact answer, but we have shown that subtraction is backwards stable. The result is entirely due to the large $\\kappa$ of the problem. If $x_1$ were only about $8\\cdot 10^{-17}$ larger though, which is a small difference relative to the true $x_1$ (a small backward error), the answer produced by the algorithm would be correct. Meaning: it found an exact solution to a nearby problem, the best thing we can hope for when dealing with an ill-conditioned problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa563ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
